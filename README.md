ETL ДЛЯ ДАННЫХ ОНЛАЙН МАГАЗИНА

Цель: визулизировать данные портала магазина.
Задачи: 
1. проанализиовать данные с источника,
2. спроектировать хранилище данных,
3. написать расчёты и алгоритмы написания витрин,
4. реализовать расчёт с помощью регулярного процесса,
5. создать дашборд, который можно посмотреть в браузере.

Stack реализации:
S3 Яндекс - Airflow - Clickhouse - Superset

Логика реализации согласно задачам:

1. в публичном Яндекс хранилище лежат файлы, содержащие Браузер и идентификаторы события, Информацию об устройстве, Данные о геопозиции, Информацию о нахождении на сайте и откуда пользователь пришёл.

2. в качестве хранилища выбрали Clickhouse, т.к. данная СУБД удобна для расчёта метрик и агрегатов. Clickhouse способен быстро расчитывать метрики на больших объемах данных хотя и жертвуя консистентностью данных. Но консистентность и не требуется.
Данные сначала грузим в RAW и через materialized view поднимаем данные на ODS, где созданы таблицы с engine ReplacingMergeTree для удаления дубликатов (см. файл lab8_ddl.sql). 
ODS используется для хранения истории, т.к. RAW таблицы грузятся методом snapshot.
Через refreshable materialized view (аналаог процедур в Clickhouse) с расписанием раз в неделю мы создаем на основе ods таблиц витрины на CDM. К cdm таблицам подключается Superset для построения dashboard.

3. для вывода агрегатов по покупкам используется следующая логика:
-определение покупки: 
3.1. находим event_id, где page_url_path = '/confirmation',
3.2. далее смотрим страницы продуктов с тем же click_id, что и event_id c page_url_path = '/confirmation',
3.3. считаем, что предыдущий пункт привёл к покупке. Т.е. если 3 страницы с продуктом имеют один click_id, что и event_id c page_url_path = '/confirmation', то значит было три покупки,
3.4. делаем допущение, что магазин не продаёт товары повседневного спроса, следовательно 1 посетитель не может дважды покупать один и тот продукт. Например, если клиент в разные дни посещал несколько раз одну страницу перед покупкой, это значит, что была только 1 покупка.

4. для автоматизации расчёта используется Airflow. В Airflow есть 2 дага. Первый даг запускает загрузку из S3 хранилища в Clickhouse. Второй даг запускает обновление refreshable materialized view. Даги запускаются руками. Расписания нет. Однако расписание обновения есть в refreshable materialized view - раз в неделю (вторник).

5. Дашборд строится в Superset (http://89.208.211.89:8088/superset/dashboard/p/jJen20pWAq4/) и имеет 11 отчётов.


Адреса:
Clickhouse http://89.208.211.89:8123/default
Airflow http://89.208.211.89:8081
Superset http://89.208.211.89:8088/lab8